\documentclass[hide notes]{beamer}
%\documentclass[only notes]{beamer}
\usepackage[english]{babel}
\usepackage{microtype}
\usepackage{environ}
\usepackage{amsmath}
\usepackage[backend=bibtex,style=phys]{biblatex}
\usepackage{doi}
\usepackage{graphicx}
\usepackage{rotating}
\usepackage{tabularx}
\usepackage{dcolumn}
\usepackage{booktabs}
\usepackage{scrextend} % for \footref
\usepackage{hyperref}
\usepackage[nameinlink,capitalize]{cleveref}
\usepackage{siunitx}
\usepackage{floatrow}
\usepackage{bm}
\usepackage{mathtools}

% style stuff
\usetheme{osu}
\hypersetup{hidelinks}
\bibliography{thesis.bib}
% https://tex.stackexchange.com/questions/10102/multiple-references-to-the-same-footnote-with-hyperref-support-is-there-a-bett/10116#10116
\crefformat{footnote}{#2\footnotemark[#1]#3}
% http://tex.stackexchange.com/questions/21913/how-can-i-change-the-footnote-line-thickness-length
\renewcommand{\footnoterule}{%
  \kern -3pt
  \hrule width \textwidth height 0.5pt
  \kern 2pt
}
% http://tex.stackexchange.com/questions/21741/how-do-i-change-footnote-font-size-in-beamer-presentation
\let\oldfootnotesize\footnotesize
\renewcommand*{\footnotesize}{\oldfootnotesize\tiny}
% http://tex.stackexchange.com/questions/169745/left-aligning-footnotes-in-beamer
\makeatletter
\renewcommand<>\beamer@framefootnotetext[1]{%
  \global\setbox\beamer@footins\vbox{%
    \hsize\framewidth
    \textwidth\hsize
    \columnwidth\hsize
    \unvbox\beamer@footins
    \reset@font\footnotesize
    \@parboxrestore
    \protected@edef\@currentlabel
         {\csname p@footnote\endcsname\@thefnmark}%
    \color@begingroup
      \uncover#2{\@makefntext{%
        \rule\z@\footnotesep\ignorespaces\parbox[t]{.9\textwidth}{#1\@finalstrut\strutbox}\vskip1sp}}%
    \color@endgroup}%
}
\makeatother

% custom commands
\DeclareMathOperator{\Tanh}{\mathbf{tanh}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
% footnote without marker
% https://tex.stackexchange.com/questions/30720/footnote-without-a-marker
\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

% toc slide
\newcommand\tocframe{%
  \frame{\tableofcontents[currentsection,hideallsubsections]}
}

% use dcolumn
\newcolumntype{d}{D{.}{.}{-1}}
\newcolumntype{e}{D{.}{.}{8}}
\newcolumntype{f}{D{.}{.}{13}}

\title{Essential Reservoir Computing}
%\subtitle{}

\author[Griffith]{Aaron Griffith}
\institute[Ohio State University]{Department of Physics\\Ohio State University}
\date[8/17/2021]{August 17, 2021}

\begin{document}

\begin{frame}[plain]
  \titlepage
\end{frame}

%\setbeamertemplate{footline}[frame number]{\tiny\insertframenumber\,/\,\inserttotalframenumber}
% https://tex.stackexchange.com/questions/434923/frame-number-does-not-work-since-ubuntu-update?noredirect=1&lq=1
\setbeamertemplate{navigation symbols}{\textcolor{osured}{\tiny\insertframenumber\,/\,\inserttotalframenumber}}

\section{Introduction}
% why RC? use word 'examples'

\frame{\tableofcontents[hideallsubsections]}

\begin{frame}{Machine Learning and Neural Networks}
  \begin{columns}
    \column{0.5\textwidth}
    \begin{itemize}
    \item neural networks are universal models
    \item learn relationship between input and output from example data
    \item very useful if this relationship is unknown, but examples are easy to find
    \item canonical example: learn how to turn images of digits into labels
    \end{itemize}
    \column{0.5\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/mnist.png}
  \end{columns}
\end{frame}

\begin{frame}{Machine Learning in Physics}
  \begin{itemize}
  \item Elsewhere:
    \begin{itemize}
    \item find masses of particles from decay data\footnote{\fullcite{lonnblad1992}}
    \item separate quark and gluon jets in $e^+ e^-$ annihilation\footnote{\fullcite{csabai1991}}
    \end{itemize}
  \item In this presentation:
    \begin{itemize}
    \item \textbf{system state inference}: given $x(t)$ and $y(t)$, find $z(t)$
    \item \textbf{chaotic system forecasting}: given the beginning of a signal $u(t)$, what happens after?
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Chaotic System Forecasting}
  \centering
  \includegraphics[width=0.7\textwidth]{figures/prediction-example}
\end{frame}

\begin{frame}{The Problem with Neural Networks}
  \begin{itemize}
  \item very expensive to train
    \begin{itemize}
    \item data hungry: require many examples
    \item deep networks take longer to train: ``vanishing gradient problem''
    \end{itemize}
  \item alternative: \textbf{Reservoir Computers}
    \begin{itemize}
    \item quick training
    \item physical interpretation
    \item physical implementation
    \end{itemize}
  \end{itemize}
\end{frame}

\section{Reservoir Computing}

\tocframe

\begin{frame}{Reservoir Computers}
  \begin{itemize}
  \item RC transforms an input signal $\bm{u}(t)$ into an output $\bm{y}(t)$
  \item input $\bm{u}(t)$ acts as driving input to dynamical system $\bm{R}$ \\ (the ``reservoir''),
    $$ \dot{\bm{r}}(t) = \bm{R}(\bm{r}, \bm{u}, t) $$
  \item reservoir response $\bm{r}(t)$ is read-out via $\bm{g}$, and linearly combined into output
    $$ \bm{y}(t) = W_\text{out}\;\bm{g}\left(\bm{r}\left(t\right)\right) $$
  \item reservoir $\bm{R}$ and read-out $\bm{g}$ fixed ahead of time, but $W_\text{out}$ is trained to fit example data
  \end{itemize}
\end{frame}

\begin{frame}{Reservoir Computers}
  \centering
  \includegraphics[width=0.8\textwidth]{figures/reservoir-generic}
\end{frame}

\begin{frame}{Training an RC}
  \begin{itemize}
  \item use example input $\bm{u}_\text{train}(t)$ and example output $\bm{y}_\text{train}(t)$
  \item example input produces example reservoir response $\bm{r}_\text{train}(t)$
  \item Goal: find $W_\text{out}$ such that
    $$ \bm{y}_\text{train}(t) \approx W_\text{out}\;\bm{g}\left(\bm{r}_\text{train}\left(t\right)\right)$$
  \item this is \emph{linear regression}! this is \emph{fast}!
  \item usually, RCs use ridge regression
  \end{itemize}
\end{frame}

\begin{frame}{Forecasting with an RC}
  \begin{itemize}
  \item for system forecasting, find $W_\text{out}$ such that
    $$ \bm{u}_\text{train}(t) \approx W_\text{out}\;\bm{g}\left(\bm{r}_\text{train}\left(t\right)\right)$$
  \item after training, feed output of RC back into itself as input
  \end{itemize}
  \centering
  \includegraphics[width=0.6\textwidth]{figures/reservoir-predict}
\end{frame}

\begin{frame}{Evaluating an RC}
  \begin{itemize}
  \item use normalized root-mean-squared error (NRMSE),
    $$ \epsilon = {\left(\sum_i\frac{\int \left| y_i(t) - y_{\text{test},i}(t) \right|^2 \;dt}{T \operatorname{Var}\left[y_{\text{test},i}(t)\right]}\right)}^{1/2} $$
  \item for chaotic system forecasting, a forecast \emph{must} diverge eventually, so only calculate for 1 Lyapunov period ($\epsilon_1$)
  \item if forecasting starts near an unstable part of the input system, small errors will amplify and $\epsilon_1$ is misleading
  \item instead, perform $m$ forecasts and average the errors
    $$ \tilde{\epsilon} = {\left( \frac{1}{m} \sum_{i=1}^m \epsilon_{1,i}^2 \right)}^{1/2} $$
  \end{itemize}
\end{frame}

\begin{frame}{Evaluating an RC}
  \centering
  \includegraphics[width=0.8\textwidth]{figures/nrmse-avg}
\end{frame}

\begin{frame}{Echo State Networks}
  \begin{itemize}
  \item use an \emph{untrained} neural network as reservoir
  \item $N$ exponential decay nodes
  \item each node has inputs drawn from $u(t)$ or other nodes
  \item summed inputs pass through activation function $f$ to drive exponential
  \end{itemize}
  \centering
  \begin{align*}
    \dot{\bm{r}}(t) &= - \gamma \bm{r}(t) + \gamma \bm{f}\left( W_r\;\bm{r}(t) + W_\text{in}\;\bm{u}(t) \right) \\
    \bm{y}(t) &= W_\text{out}\;\bm{g}\left(\bm{r}(t)\right)
  \end{align*}
\end{frame}

\begin{frame}{Echo State Networks}
  \centering
  \includegraphics[width=0.8\textwidth]{figures/reservoir}
\end{frame}

\begin{frame}{Constructing ESNs}
  \begin{itemize}
  \item $\gamma$ (node decay rate) is tuned to match input timescale
  \item $f$ (activation function) is traditionally $\tanh$
    \vspace{2em}
  \item $W_r$ and $W_\text{in}$ chosen \emph{randomly}
    \begin{itemize}
    \item $k$, recurrent inputs for each node
    \item $\rho_r$, spectral radius of $W_r$
    \item $\sigma$, probability of external input per node
    \item $\rho_\text{in}$, scale of $W_\text{in}$
    \end{itemize}
    \vspace{2em}
  \item What reservoirs work well? What ESN parameters work well?
  \item \emph{How much of this is necessary?}
  \end{itemize}
\end{frame}

\section{Low-Connectivity Reservoirs}

\tocframe

\begin{frame}{Exploring ESN Parameters}
  \begin{columns}
    \column{0.6\textwidth}
    \begin{itemize}
    \item use Bayesian optimization to find ESN parameters that work well for Lorenz system forecasting
    \item strange result: optimized parameters often had $k = 1$
    \item ESNs with $k = 1$ have very simple structure, but perform as well as $k > 1$
    \item inspired by this, I also look at three other simple structures...
    \end{itemize}
    \column{0.4\textwidth}
    \centering
    \includegraphics[width=1.0\textwidth]{figures/topology-b}
  \end{columns}
\end{frame}

\begin{frame}{Structure of Low-Connectivity Reservoirs}
    \begin{columns}
      \column{0.4\textwidth}
      \small
      (a) general construction \\
      (b) $k = 1$, single cycle \\
      (c) $k = 1$, cut cycle \\
      (d) \emph{simple cycle reservoir}\footnote[frame]{\fullcite{rodan2011}\label{fn:rodan}} \\
      (e) \emph{delay line reservoir}\cref{fn:rodan} \\~\\
      In all cases, networks with more than one connected component are discarded and regenerated
      \column{0.6\textwidth}
      \centering
      \includegraphics[width=1.0\textwidth]{figures/topology}
  \end{columns}
\end{frame}

\begin{frame}{Training, Forecasting, and Evaluation}
  \begin{itemize}
  \item for each structure, I search for the parameters to optimize $\tilde{\epsilon}$ on Lorenz forecasting
  \item for each set of trial parameters...
    \begin{itemize}
    \item build a 100-node ESN with those parameters
    \item train for 100 units of time ($\Delta t = 0.01$)
    \item evaluate $\tilde{\epsilon}$ by averaging $50$ $\epsilon_1$ errors
    \end{itemize}
    \vspace{2em}
  \item in the end, have one ESN from each structure type that performs well
  \end{itemize}
\end{frame}

\begin{frame}{ESN Symmetries and their Consequences}
  \begin{itemize}
  \item when $f = \tanh$ and $\bm{g}(\bm{r}) = \bm{r}$, ESN equation has $\bm{r} \rightarrow -\bm{r}$ symmetry, and therefore $\bm{y} \rightarrow -\bm{y}$ symmetry
    \begin{align*}
      \dot{\bm{r}}(t) &= - \gamma \bm{r}(t) + \gamma \bm{f}\left( W_r\;\bm{r}(t) + W_\text{in}\;\bm{u}(t) \right) \\
      \bm{y}(t) &= W_\text{out}\;\bm{g}\left(\bm{r}(t)\right)
    \end{align*}
  \item Lorenz does \emph{not}!
  \item Solution: break the symmetry by using a non-linear $\bm{g}$\footnote{\fullcite{herteux2020}}
    \begin{equation*}
      g_i(\bm{r}) = \begin{cases}
        r_i & \text{if } i \leq N / 2, \\
        r_i^2 & \text{if } i > N / 2.
      \end{cases}
    \end{equation*}
    \end{itemize}
\end{frame}

\begin{frame}{ESN Symmetries and their Consequences}
  \centering
  \includegraphics[width=0.7\textwidth]{figures/lorenz-symmetry}
\end{frame}

\begin{frame}{$\epsilon_1$ vs. $\tilde{\epsilon}$}
  \begin{center}
    \includegraphics[width=0.7\textwidth]{figures/epsilon-failure}
  \end{center}

  {\small
    (i) overemphasizes lobe where $\epsilon_1$ resides \\
    (ii) good short-term forecast, but falls into periodic orbit as $t \rightarrow \inf$}
\end{frame}

\begin{frame}{Results}
  \centering
  \begin{tabularx}{\linewidth}{l l@{\extracolsep{\fill}} l}
    & Structure & $\tilde{\epsilon}$ \\
    \hline
    (a) & Any $k$ & 0.022 $\pm$ 0.004 \\
    (b) & $k = 1$ with cycle & 0.024 $\pm$ 0.005 \\
    (c) & $k = 1$ no cycle & 0.028 $\pm$ 0.005$^*$ \\
    (d) & cycle & 0.023 $\pm$ 0.008 \\
    (e) & line & 0.024 $\pm$ 0.003$^*$ \\
  \end{tabularx}

  \vspace{3em}
  $^*$ \emph{no recurrence in network!}
\end{frame}

\begin{frame}{Results}
  \centering
  \includegraphics[width=0.7\textwidth]{figures/lowk-attractors}
\end{frame}

\begin{frame}{ESN Variation}
  \begin{itemize}
    \item ESN construction is random -- what if I re-generate a network with the same parameters as best performers?
  \end{itemize}
  \centering
  \includegraphics[width=0.7\textwidth]{figures/epsilon-distribution}
\end{frame}

\begin{frame}{Conclusion}
  \begin{itemize}
  \item $\tilde{\epsilon}$ captures forecasting quality more reliably than $\epsilon_1$
  \item very simple ESNs can perform as well as ESNs with more complicated connectivity
  \item this is still true even when the network is a single cycle, or a line
  \item however, well-performing simple ESNs are harder to find
    \vspace{3em}
  \item If line networks work well... maybe all the network does is combine time-delayed copies of the input?
  \end{itemize}
\end{frame}

\section{RCs without Reservoirs: NVARs}

\tocframe

\section{NVARs in Practice}

\tocframe

\section{Conclusion}

\tocframe

\begin{frame}
  \begin{center}
    {\usebeamercolor[fg]{frametitle}{\Large Thank you!}}

    Questions?
  \end{center}

  \note{o7}
\end{frame}

\end{document}

